{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g--QUuESs7hi"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import nltk\n",
        "from nltk import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "import numpy as np\n",
        "import numpy.linalg as lin\n",
        "import pandas as pd\n",
        "import unicodedata\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import pickle\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps=PorterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWEZwunVtI2L",
        "outputId": "c9a687f0-152a-4932-cd5f-178dfb7b6e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v = KeyedVectors.load_word2vec_format('/content/w2v.bin', binary=True)"
      ],
      "metadata": {
        "id": "F3-Adu-Dx76G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PREPROCESS"
      ],
      "metadata": {
        "id": "jpyyVn_Ei5r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_accents(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
      ],
      "metadata": {
        "id": "9lqtxDFoyQP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_special_characters(text):\n",
        "    regex = re.compile('[^a-zA-Z\\s]')\n",
        "    text_returned = re.sub(regex,' ',text)\n",
        "    return text_returned"
      ],
      "metadata": {
        "id": "QjU_VPxiySj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_embedding(word):\n",
        "    word=word.lower()\n",
        "    try:\n",
        "        return w2v.get_vector(word)\n",
        "    except:\n",
        "        return np.array([0.0]*300)"
      ],
      "metadata": {
        "id": "1Kbtd310yUT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_embedding(sentence):\n",
        "    sentence=strip_accents(sentence)\n",
        "    sentence=remove_special_characters(sentence)\n",
        "    words=word_tokenize(sentence)\n",
        "    if len(words)>0:\n",
        "        words=[word  for word in words if word not in stop_words]\n",
        "        sentence_embedding=[word_embedding(word) for word in words]\n",
        "        return np.array(list(map(lambda x: sum(x)/len(x), zip(*sentence_embedding))))\n",
        "    return np.array([0]*300)"
      ],
      "metadata": {
        "id": "5xhA6X5jyWKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/English.csv')"
      ],
      "metadata": {
        "id": "eDCpVPCyyYCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence by Sentence"
      ],
      "metadata": {
        "id": "mVeR4jsHydhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verse_embeddings_sentence=[]\n",
        "verse_embeddings_max=[]\n",
        "verse_embeddings_mean=[]\n",
        "for i in range(len(data)):\n",
        "    text=data.loc[i]['Commentary']\n",
        "    text=strip_accents(text)\n",
        "    sentences=sent_tokenize(text)\n",
        "    embeddings=[sentence_embedding(sentence) for sentence in sentences]\n",
        "    #sentence\n",
        "    verse_embeddings_sentence.append(embeddings)\n",
        "    #Max Pooling\n",
        "    norms=[lin.norm(i) for i in embeddings]\n",
        "    index=norms.index(max(norms))\n",
        "    verse_embeddings_max.append(embeddings[index])\n",
        "    #Mean Pooling\n",
        "    embeddings=np.array(list(map(lambda x: sum(x)/len(x), zip(*embeddings))))\n",
        "    verse_embeddings_mean.append(embeddings)"
      ],
      "metadata": {
        "id": "Ijf385JgybbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Whole Sentence"
      ],
      "metadata": {
        "id": "mUyNh3uey0rF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verse_embeddings_whole=[]\n",
        "for i in range(len(data)):\n",
        "    text=data.loc[i]['Commentary']\n",
        "    sentence=strip_accents(text)\n",
        "    embeddings=sentence_embedding(sentence)\n",
        "    verse_embeddings_whole.append(embeddings)"
      ],
      "metadata": {
        "id": "7d9C8JwSylfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file=open('sentence.pkl','wb')\n",
        "pickle.dump(verse_embeddings_sentence,file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "vnfFEqcQy7Lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file=open('whole.pkl','wb')\n",
        "pickle.dump(verse_embeddings_whole,file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "UiydaVf9zHqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file=open('max.pkl','wb')\n",
        "pickle.dump(verse_embeddings_max,file)\n",
        "file.close()\n",
        "file=open('mean.pkl','wb')\n",
        "pickle.dump(verse_embeddings_mean,file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "ura6Mo2AzIGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model training\n"
      ],
      "metadata": {
        "id": "tmwcyO0t_mBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def load_data(file_path):\n",
        "    data = pd.read_csv('/content/English.csv')\n",
        "    return data\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text), re.I|re.A)  # Remove non-alphabetic characters\n",
        "    text = text.lower() # Lowercase\n",
        "    tokens = simple_preprocess(text, deacc=True, min_len=3) # Tokenize\n",
        "    stop_words = set(stopwords.words('english')) # Remove Stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "file_path = '/content/English.csv'\n",
        "data = load_data(file_path)\n",
        "data['tokens'] = data['Commentary'].astype(str).apply(preprocess_text)\n",
        "sentences = data['tokens'].tolist()\n",
        "\n",
        "# Word2Vec parameters\n",
        "vector_size = 300\n",
        "window = 5\n",
        "min_count = 1\n",
        "workers = 4\n",
        "sg = 0\n",
        "epochs = 10\n",
        "\n",
        "model = Word2Vec(sentences, vector_size=vector_size, window=window, min_count=min_count, workers=workers, sg=sg, epochs=epochs)\n",
        "\n",
        "model.save(\"bhagavad_gita_word2vec.model\")\n",
        "\n",
        "model.wv.save_word2vec_format('w2v.bin', binary=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmO7C6Fw3kcM",
        "outputId": "11bf49a1-a74c-417c-a998-0733abf85345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0_7yqAdiRt7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}